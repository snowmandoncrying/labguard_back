import os
import uuid
import time
import re
import io
from fastapi import UploadFile
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_core.documents import Document
from dotenv import load_dotenv
# import pytesseract
from pdf2image import convert_from_path
import base64
from typing import List
import json 

from PyPDF2 import PdfReader
from PIL import Image
from openai import OpenAI
from google.generativeai import configure, GenerativeModel

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")

if not OPENAI_API_KEY:
    raise RuntimeError("OPENAI_API_KEY not found in environment variables.")
if not GOOGLE_API_KEY:
    raise RuntimeError("GOOGLE_API_KEY not found in environment variables.")

client = OpenAI(api_key=OPENAI_API_KEY)
configure(api_key=GOOGLE_API_KEY)


CHROMA_DIR = "./chroma_db"  
POPLER_PATH = r"C:\Users\201-13\Documents\poppler-24.08.0\Library\bin"

# pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"



# =====================
# ì²­í¬ í•„í„°ë§ í•¨ìˆ˜ ì •ì˜
# =====================
# def filter_chunk(text: str) -> bool:
#     """
#     íŠ¹ìˆ˜ë¬¸ì/ìˆ˜ì‹/ê¹¨ì§„ ë¬¸ìê°€ í¬í•¨ëœ chunkë¥¼ í•„í„°ë§í•˜ëŠ” í•¨ìˆ˜.
#     - í•œê¸€, ì˜ì–´, ìˆ«ì, ê³µë°±, ì¼ë°˜ì ì¸ ë¬¸ì¥ë¶€í˜¸(.,;:!?()-[]/) ë§Œ í—ˆìš©
#     - ë‚˜ë¨¸ì§€ íŠ¹ìˆ˜ë¬¸ì/ìˆ˜ì‹/ê¹¨ì§„ ë¬¸ìëŠ” ì œì™¸
#     """
#     # í—ˆìš© ë¬¸ì: í•œê¸€, ì˜ë¬¸, ìˆ«ì, ê³µë°±, ì¼ë°˜ ë¬¸ì¥ë¶€í˜¸
#     allowed_pattern = r'[^\uAC00-\uD7A3a-zA-Z0-9 .,;:!?()\[\]\-/]'  # í—ˆìš© ì™¸ ë¬¸ì
#     # íŠ¹ìˆ˜ë¬¸ì/ê¹¨ì§„ë¬¸ì/ìˆ˜ì‹ì´ ìˆìœ¼ë©´ False
#     if re.search(allowed_pattern, text):
#         return False
#     return True

# ê¹¨ì§„ í…ìŠ¤íŠ¸ íŒë³„
def is_broken_or_missing(text: str) -> bool:
    if not text.strip():
        return True
    broken_chars = text.count("â–¡") + text.count("ï¿½")
    ratio = broken_chars / len(text)
    return ratio > 0.05 or len(text.strip()) < 10

# ê·¸ë¦¼/í‘œ ìº¡ì…˜ í¬í•¨ ì—¬ë¶€
def has_figure_or_table_caption(text: str) -> bool:
    patterns = ["ê·¸ë¦¼ \d+", "í‘œ \d+", r"\[ê·¸ë¦¼ \d+\]", r"\[í‘œ \d+\]"]
    return any(re.search(pat, text) for pat in patterns)

# ëˆ„ë½ í˜ì´ì§€ í™•ì¸
def get_missing_page_numbers(total_pages: int, parsed_docs: list) -> set:
    parsed_page_nums = set(doc.metadata.get("page", -1) for doc in parsed_docs)
    return set(range(1, total_pages + 1)) - parsed_page_nums

# ì²­í¬ í•„í„°ë§
def filter_chunk(text: str) -> bool:
    text = text.strip()
    if len(text) < 5:
        return False
    valid_chars = re.findall(r'[\uAC00-\uD7A3a-zA-Z0-9 .,;:!?()\[\]\-/]', text)
    return len(valid_chars) / len(text) > 0.5

# ì œë¯¸ë‚˜ì´ ëª¨ë¸ í˜¸ì¶œ
def call_vision_model_with_gemini(image: Image.Image) -> str:
    import google.generativeai as genai
    prompt = """
ë‹¤ìŒ ì´ë¯¸ì§€ë¥¼ ì‚¬ëŒì´ ì§ì ‘ ë³´ëŠ” ê²ƒì²˜ëŸ¼ ì‹œê°ì ìœ¼ë¡œ ì„¤ëª…í•´ ì£¼ì„¸ìš”.

- ë„í˜•ì˜ ëª¨ì–‘(ì˜ˆ: ê³¡ì„ , ì§ì„ , íŒŒì´í”„ í˜•íƒœ ë“±), ë¼ë²¨(hâ‚, hâ‚‚ ë“±), í™”ì‚´í‘œ ë°©í–¥, ì—°ê²° ê´€ê³„ ë“±ì„ êµ¬ì²´ì ìœ¼ë¡œ ë¬˜ì‚¬í•´ ì£¼ì„¸ìš”.
- ì´ë¯¸ì§€ì— í¬í•¨ëœ ìˆ˜ì‹ì´ë‚˜ ê¸°í˜¸ëŠ” í•´ì„í•˜ì§€ ë§ê³  **í…ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ** ë³´ì—¬ ì£¼ì„¸ìš”. ì˜ˆ: \\( R_A = -k[A] \\
- "ê·¸ë¦¼ 5", "í‘œ 3"ê³¼ ê°™ì€ ìº¡ì…˜ì´ë‚˜ ë²ˆí˜¸ë„ **ê·¸ëŒ€ë¡œ ì¶”ì¶œ**í•´ì„œ ë§í•´ ì£¼ì„¸ìš”.
- êµ¬ì„± ìš”ì†Œë“¤ì˜ ìƒëŒ€ì  ìœ„ì¹˜(ì˜ˆ: ì™¼ìª½ íƒ±í¬, ì˜¤ë¥¸ìª½ íŒŒì´í”„ ë“±)ë¥¼ ëª…í™•íˆ ì„¤ëª…í•´ ì£¼ì„¸ìš”.
- ì‚¬ëŒì´ ê·¸ë¦¼ì„ ë³´ê³  ì„¤ëª…í•˜ë“¯, **êµ¬ì¡°ì™€ íë¦„** ìœ„ì£¼ë¡œ ë§í•´ ì£¼ì„¸ìš”.

â€» ì„¤ëª…ì€ í•œêµ­ì–´ë¡œ í•´ì£¼ì„¸ìš”.
"""
    model = genai.GenerativeModel("gemini-1.5-pro-latest")
    response = model.generate_content([prompt, image])
    return response.text

# === ì‹¤í—˜ ì œëª© ì°¾ê¸° & ID ë¶€ì—¬ ===
def extract_experiment_titles(chunks: List[Document]) -> List[int]:
    """
    ë¬¸ì„œ ì²­í¬ì—ì„œ ì‹¤í—˜ ì œëª©(ì„¹ì…˜ ì‹œì‘) ì¸ë±ìŠ¤ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    # sampled_chunks_for_llm ë¦¬ìŠ¤íŠ¸ë¥¼ ë¨¼ì € ì •ì˜
    sampled_chunks_list = [] 
    # ì „ì²´ ì²­í¬ì˜ í…ìŠ¤íŠ¸ ì–‘ì´ ë§ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ì¼ë¶€ë§Œ ìƒ˜í”Œë§í•˜ì—¬ LLMì— ì „ë‹¬í•©ë‹ˆë‹¤.
    # ì—¬ê¸°ì„œëŠ” ì²˜ìŒ 20ê°œ ì²­í¬ì™€ ì´í›„ 20ê°œ ì²­í¬ë§ˆë‹¤ í•˜ë‚˜ì”© ìƒ˜í”Œë§í•©ë‹ˆë‹¤.
    for i, chunk in enumerate(chunks):
        if i < 50 or (i % 10 == 0 and i > 0): # ì²˜ìŒ 50ê°œ + ì´í›„ 10ê°œë§ˆë‹¤ 1ê°œì”© ìƒ˜í”Œë§
            # ì²­í¬ ë‚´ìš©ì„ 500ìë¡œ ì œí•œ (í† í° ë¹„ìš© ê´€ë¦¬)
            preview_content = chunk.page_content[:1000]
            sampled_chunks_list.append(f"CHUNK_{i}:\n{preview_content}\n---\n")

    # ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ í•©ì³ì„œ LLMì— ì „ë‹¬í•  ìµœì¢… í…ìŠ¤íŠ¸ ìƒì„±
    full_text_sample_for_llm = "\n".join(sampled_chunks_list)
    
    prompt = f"""
    ë‹¹ì‹ ì€ ë‹¤ì–‘í•œ ê¸°ìˆ  ë§¤ë‰´ì–¼ì—ì„œ "ì‹¤í—˜" ì„¹ì…˜ì˜ ì‹œì‘ì ì„ ì •í™•í•˜ê²Œ ì‹ë³„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    ì•„ë˜ëŠ” ë¬¸ì„œì˜ ê° ì²­í¬ ë‚´ìš©(ì¼ë¶€ ìƒ˜í”Œ)ì…ë‹ˆë‹¤. ì´ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë‹¤ìŒ ì§€ì‹œì— ë”°ë¼ **"ì£¼ìš” ì‹¤í—˜" ì„¹ì…˜ì˜ ì‹œì‘ ì¸ë±ìŠ¤**ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”.

    **"ì£¼ìš” ì‹¤í—˜"ì˜ ì •ì˜:**
    - ì´ ë§¤ë‰´ì–¼ì—ì„œ í•˜ë‚˜ì˜ ë…ë¦½ì ì´ê³  ì™„ê²°ëœ **ì‹¤í—˜ ê³¼ì •, ë°©ë²•ë¡ , ë˜ëŠ” ì—°êµ¬ ì£¼ì œ**ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìµœìƒìœ„ ë ˆë²¨ì˜ ì„¹ì…˜ì…ë‹ˆë‹¤.
    - ì¼ë°˜ì ìœ¼ë¡œ ëª©ì°¨ì— ë‚˜íƒ€ë‚˜ëŠ” í•­ëª©ì´ê±°ë‚˜, ë¬¸ì„œ ë‚´ì—ì„œ ìƒˆë¡œìš´ ì¥(Chapter), íŒŒíŠ¸(Part) ë˜ëŠ” í° ì„¹ì…˜ì˜ ì‹œì‘ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
    - ë³´í†µ "ì‹¤í—˜ 1", "ì œ II ì¥", "Part A: [ì‹¤í—˜ëª…]", ë˜ëŠ” êµµì€ ê¸€ì”¨ì™€ í° í°íŠ¸ë¡œ ëœ ì œëª© ë“±ì´ ì´ì— í•´ë‹¹í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

    **"ì£¼ìš” ì‹¤í—˜"ì´ ì•„ë‹Œ ê²½ìš° (ìƒˆë¡œìš´ ì‹¤í—˜ìœ¼ë¡œ ê°„ì£¼í•˜ì§€ ì•ŠìŒ):**
    - ê° "ì£¼ìš” ì‹¤í—˜" ë‚´ë¶€ì— ìˆëŠ” í•˜ìœ„ ì†Œì œëª©ë“¤ (ì˜ˆ: "1. ì„œë¡ ", "2. ì‹¤í—˜ ì´ë¡ ", "3. ì‹¤í—˜ ê¸°êµ¬", "4. ì‹¤í—˜ ìˆœì„œ", "5. ê²°ê³¼ ë° í† ì˜", "6. ì°¸ê³ ë¬¸í—Œ").
    - í•˜ìœ„ ì ˆ (ì˜ˆ: "3.1 ì‹œì•½ ì¤€ë¹„").
    - ë‹¨ìˆœíˆ í˜ì´ì§€ ë²ˆí˜¸, ì¥/ì ˆ ë²ˆí˜¸ë§Œ ìˆëŠ” ë¼ì¸.
    - 'Abstract', 'ê°œìš”', 'ë„ì…', 'ê²°ë¡ ' ë“±ì€ íŠ¹ì • ì‹¤í—˜ì˜ ì¼ë¶€ì´ì§€, ë…ë¦½ì ì¸ ìƒˆ ì‹¤í—˜ì´ ì•„ë‹™ë‹ˆë‹¤.
    - í‘œë‚˜ ê·¸ë¦¼ì˜ ìº¡ì…˜, ì£¼ì„, ë¶€ë¡, ìƒ‰ì¸ ë“±.

    **ë¬¸ì„œ ì²­í¬ ë‚´ìš© (ìƒ˜í”Œ):**
    {full_text_sample_for_llm}

    ---
    **ìµœì¢… ì§€ì‹œì‚¬í•­:**
    ìœ„ ê¸°ì¤€ì— ë”°ë¼ ì´ ë¬¸ì„œì˜ ëª¨ë“  **"ì£¼ìš” ì‹¤í—˜" ì„¹ì…˜ì´ ì‹œì‘ë˜ëŠ” ì²­í¬ì˜ ì¸ë±ìŠ¤(CHUNK_X:)**ë¥¼ ëª¨ë‘ ì°¾ì•„ì£¼ì„¸ìš”.
    ë‹µë³€ì€ **ì˜¤ì§ JSON ë°°ì—´ í˜•íƒœ**ë¡œ, ê° ì •ìˆ˜ëŠ” í•´ë‹¹ "ì£¼ìš” ì‹¤í—˜" ì„¹ì…˜ì´ ì‹œì‘ë˜ëŠ” ì²­í¬ì˜ ì¸ë±ìŠ¤ì—¬ì•¼ í•©ë‹ˆë‹¤.
    **ë°˜ë“œì‹œ ì´ JSON í˜•ì‹ë§Œ ë°˜í™˜í•´ì•¼ í•©ë‹ˆë‹¤.** ë§Œì•½ ì‹ë³„ëœ ì£¼ìš” ì‹¤í—˜ ì„¹ì…˜ì´ ì—†ë‹¤ë©´ ë¹ˆ ë°°ì—´ `[]`ì„ ë°˜í™˜í•´ì£¼ì„¸ìš”.

    **ì˜ˆì‹œ (ë¬¸ì„œì— ë”°ë¼ ì‹¤ì œ ì¸ë±ìŠ¤ëŠ” ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤):**
    `[2, 15, 30, 45]`

    # ì œã…”ì•½ì‚¿í•­
    - ì£¼ì˜ì‚¬í•­ì€ ë¬´ì‹œí•˜ê³  ìµœëŒ€í•œ ë§ì€ ì‹¤í—˜ì„ ì°¾ì•„ì£¼ì„¸ìš”.
    """


    # --- LLM í˜¸ì¶œ ---
    response = client.chat.completions.create(
        model="gpt-4.1-mini", 
        messages=[
            {"role": "user", "content": [{"type": "text", "text": prompt}]}
        ],
        max_tokens=256, 
        temperature=0.0, # ëª…í™•í•œ ì‚¬ì‹¤ ì¶”ì¶œì„ ìœ„í•´ ë‚®ì€ ì˜¨ë„ ìœ ì§€
    )
    
    llm_output_str = response.choices[0].message.content
    llm_indices = json.loads(llm_output_str)

    print(f"âœ… LLMì´ ì‹ë³„í•œ ì‹¤í—˜ ì œëª© ì¸ë±ìŠ¤: {llm_indices}")
    
    # LLMì´ ì•„ë¬´ê²ƒë„ ì°¾ì§€ ëª»í–ˆì„ ë•Œì˜ ì²˜ë¦¬ (ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜ì„ ê°€ì •)
    if not llm_indices:
        print("âš ï¸ LLMì´ ì£¼ìš” ì‹¤í—˜ ì œëª©ì„ ì‹ë³„í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ëª¨ë“  ì²­í¬ì— ë‹¨ì¼ experiment_idë¥¼ í• ë‹¹í•©ë‹ˆë‹¤.")
        # ì´ ê²½ìš°, assign_experiment_ids í•¨ìˆ˜ì—ì„œ exp01 í•˜ë‚˜ê°€ í• ë‹¹ë  ê²ƒì…ë‹ˆë‹¤.
        # ëª…ì‹œì ìœ¼ë¡œ [0]ì„ ë°˜í™˜í•˜ì—¬ ìµœì†Œí•œ ì²« ì²­í¬ë¶€í„° exp01ì´ ë˜ë„ë¡ í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
        return [0] 
    
    return sorted(list(set(llm_indices)))

def assign_experiment_ids(chunks: List[Document], manual_id: str) -> List[Document]:
    """
    ì²­í¬ì— experiment_id ë©”íƒ€ë°ì´í„°ë¥¼ í• ë‹¹í•©ë‹ˆë‹¤.
    """
    title_indexes = extract_experiment_titles(chunks) # LLM ë˜ëŠ” ì •ê·œí‘œí˜„ì‹ ì‚¬ìš©

    # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
    title_indexes = sorted(list(set(title_indexes)))

    # ì„¹ì…˜ ì‹œì‘ ì¸ë±ìŠ¤ ëª©ë¡ ìƒì„± (0ë¶€í„° ì‹œì‘í•˜ë„ë¡ ë³´ì¥)
    section_start_indices = [0]
    if 0 not in title_indexes: # 0ì´ ì´ë¯¸ ì œëª© ì¸ë±ìŠ¤ì— ì—†ìœ¼ë©´ ì¶”ê°€
        section_start_indices.extend(title_indexes)
    else: # 0ì´ ì´ë¯¸ ìˆìœ¼ë©´, title_indexesë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
        section_start_indices = title_indexes
    
    # ë‹¤ì‹œ í•œë²ˆ ì¤‘ë³µ ì œê±° ë° ì •ë ¬
    section_start_indices = sorted(list(set(section_start_indices)))

    # ê° ì„¹ì…˜ì— experiment_id í• ë‹¹
    for i in range(len(section_start_indices)):
        start_chunk_idx = section_start_indices[i]
        # ë‹¤ìŒ ì„¹ì…˜ ì‹œì‘ ì „ê¹Œì§€ ë˜ëŠ” ë¬¸ì„œ ëê¹Œì§€
        end_chunk_idx = section_start_indices[i+1] if i+1 < len(section_start_indices) else len(chunks)
        
        exp_id = f"{manual_id}_exp{i+1:02}" # ì‹¤í—˜ IDëŠ” 01ë¶€í„° ì‹œì‘

        for chunk_idx in range(start_chunk_idx, end_chunk_idx):
            # ì²­í¬ ì¸ë±ìŠ¤ê°€ ìœ íš¨í•œ ë²”ìœ„ ë‚´ì— ìˆëŠ”ì§€ í™•ì¸
            if chunk_idx < len(chunks):
                chunks[chunk_idx].metadata["experiment_id"] = exp_id
                
    return chunks

async def embed_pdf_manual(file: UploadFile, manual_type: str = "UNKNOWN", user_id: int = None) -> dict:
    import tempfile, shutil
    temp_dir = tempfile.mkdtemp()
    temp_path = os.path.join(temp_dir, file.filename)
    try:
        with open(temp_path, "wb") as f:
            content = await file.read()
            f.write(content)
        # 1. manual_id ìƒì„± (uuid)
        manual_id = str(uuid.uuid4())
        print(f"ğŸ‰ ìƒˆ ë§¤ë‰´ì–¼ ID ìƒì„±: {manual_id}")
        # 2. PyPDFLoaderë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì²­í‚¹
        loader = PyPDFLoader(temp_path)
        docs = loader.load()
        splitter = RecursiveCharacterTextSplitter(chunk_size=1500, chunk_overlap=200)
        split_docs = splitter.split_documents(docs)
        # 3. ì¼ë°˜ chunkì— ë©”íƒ€ë°ì´í„° ë¶€ì—¬
        pdf_chunks = []
        vision_page_candidates = set()

        for idx, doc in enumerate(split_docs):
            # if not filter_chunk(doc.page_content):
            #     continue  # íŠ¹ìˆ˜ë¬¸ì/ìˆ˜ì‹/ê¹¨ì§„ ë¬¸ìê°€ í¬í•¨ëœ ì²­í¬ëŠ” ì €ì¥í•˜ì§€ ì•ŠìŒ
            page_num = doc.metadata.get("page", 1)
            content = doc.page_content.strip()

            if is_broken_or_missing(content):
                vision_page_candidates.add(page_num)
                continue

            if has_figure_or_table_caption(content):
                vision_page_candidates.add(page_num)

            if not filter_chunk(content):
                continue

            meta = {
                "manual_id": manual_id,
                "manual_type": manual_type,
                "page_num": page_num,
                "chunk_idx": idx,
                "source": "pdf",
                "filename": file.filename,
                "uploaded_at": int(time.time()),
                "user_id": user_id
            }
            pdf_chunks.append(Document(page_content=content, metadata=meta))
            # existing_texts.add(content)
            
        total_pages = len(PdfReader(temp_path).pages)
        missing_pages = get_missing_page_numbers(total_pages, split_docs)
        vision_page_candidates.update(missing_pages)

        images = convert_from_path(temp_path, poppler_path=POPLER_PATH)
        vision_docs = []

        for page_num in sorted(vision_page_candidates):
            if page_num - 1 < len(images):
                image = images[page_num - 1]
                vision_text = call_vision_model_with_gemini(image)

                # ë¹„ì „ ëª¨ë¸ì—ì„œ ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ë„ í•„í„°ë§
                if not filter_chunk(vision_text):
                    continue

                meta = {
                    "manual_id": manual_id,
                    "manual_type": manual_type,
                    "page_num": page_num,
                    "chunk_idx": len(pdf_chunks) + len(vision_docs),
                    "source": "gemini",
                    "chunk_type": "vision_extracted",
                    "filename": file.filename,
                    "uploaded_at": int(time.time()),
                    "user_id": user_id
                }
                vision_docs.append(Document(page_content=vision_text, metadata=meta))

        # existing_texts = set(doc.page_content.strip() for doc in split_docs)
        # for idx, img in enumerate(images):
        #     ocr_text = pytesseract.image_to_string(img, lang='kor+eng').strip()
        #     if not ocr_text or ocr_text in existing_texts:
        #         continue
        #     if not filter_chunk(ocr_text):
        #         continue  # íŠ¹ìˆ˜ë¬¸ì/ìˆ˜ì‹/ê¹¨ì§„ ë¬¸ìê°€ í¬í•¨ëœ ì²­í¬ëŠ” ì €ì¥í•˜ì§€ ì•ŠìŒ
        #     meta = {
        #         "manual_id": manual_id,
        #         "manual_type": manual_type,
        #         "page_num": idx + 1,
        #         "chunk_idx": idx,
        #         "source": "ocr",
        #         "filename": file.filename,
        #         "uploaded_at": int(time.time())
        #     }
        #     ocr_docs.append(Document(page_content=ocr_text, metadata=meta))
        #     print("âœ… [5] OCR í†µê³¼í•œ ì²­í¬ ìˆ˜:", len(ocr_docs))
        
        # ëª¨ë“  chunkì— experiment_id í• ë‹¹
        all_docs = pdf_chunks + vision_docs
        all_docs = assign_experiment_ids(all_docs, manual_id)
        # í• ë‹¹ëœ ê³ ìœ  experiment_id ëª©ë¡ ì¶”ì¶œ
        assigned_experiment_ids = sorted(list(set(doc.metadata.get("experiment_id") for doc in all_docs if "experiment_id" in doc.metadata)))
        #ë²¡í„°dbì €ì¥
        embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
        vectorstore = Chroma.from_documents(all_docs, embeddings, persist_directory=CHROMA_DIR)
        vectorstore.persist()
        return {
            "message": "PDF ì„ë² ë”© ë° ì €ì¥ ì™„ë£Œ",
            "manual_id": manual_id,
            "pdf_chunks": len(pdf_chunks),
            "ocr_chunks": len(vision_docs),
            "total_chunks": len(all_docs),
            "experiment_ids": assigned_experiment_ids
        }
    finally:
        try:
            shutil.rmtree(temp_dir)
        except Exception:
            pass 