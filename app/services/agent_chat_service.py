import os
from typing import List, Dict, Optional
from dotenv import load_dotenv, find_dotenv
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OpenAIEmbeddings
from langchain_openai import ChatOpenAI
from langchain.agents import initialize_agent, Tool, AgentType
from langchain_core.documents import Document
from pydantic import BaseModel, Field
import time
import json
from datetime import datetime
from app.schemas.query import ManualSearchInput

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
dotenv_path = find_dotenv()
if dotenv_path:
    load_dotenv(dotenv_path=dotenv_path)
else:
    load_dotenv()

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
CHROMA_DIR = "./chroma_db"
EXPERIMENT_LOG_FILE = "./experiment_logs.json"

# ì‹¤í—˜ ë¡œê·¸ ê´€ë¦¬ í´ë˜ìŠ¤
class ExperimentLogger:
    def __init__(self, log_file: str = EXPERIMENT_LOG_FILE):
        self.log_file = log_file
        self.experiments = self.load_experiments()
    
    def load_experiments(self) -> List[Dict]:
        try:
            if os.path.exists(self.log_file):
                with open(self.log_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
        except Exception as e:
            print(f"ë¡œê·¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return []
    
    def save_experiments(self):
        try:
            with open(self.log_file, 'w', encoding='utf-8') as f:
                json.dump(self.experiments, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ë¡œê·¸ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def add_experiment_log(self, user_id: str, content: str, experiment_type: str = "progress"):
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "user_id": user_id,
            "type": experiment_type,  # progress, result, observation, issue
            "content": content
        }
        self.experiments.append(log_entry)
        self.save_experiments()
        return log_entry
    
    def get_user_experiments(self, user_id: str, limit: int = 10) -> List[Dict]:
        user_logs = [exp for exp in self.experiments if exp.get("user_id") == user_id]
        return user_logs[-limit:]
    
    def generate_report(self, user_id: str) -> str:
        user_logs = self.get_user_experiments(user_id, limit=50)
        if not user_logs:
            return "ê¸°ë¡ëœ ì‹¤í—˜ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        report = f"=== ì‹¤í—˜ ì§„í–‰ ë³´ê³ ì„œ (ì´ {len(user_logs)}ê°œ í•­ëª©) ===\n\n"
        
        # íƒ€ì…ë³„ ë¶„ë¥˜
        progress_logs = [log for log in user_logs if log.get("type") == "progress"]
        result_logs = [log for log in user_logs if log.get("type") == "result"]
        observation_logs = [log for log in user_logs if log.get("type") == "observation"]
        issue_logs = [log for log in user_logs if log.get("type") == "issue"]
        
        if progress_logs:
            report += "ğŸ“‹ **ì‹¤í—˜ ì§„í–‰ ìƒí™©:**\n"
            for log in progress_logs[-5:]:  # ìµœê·¼ 5ê°œë§Œ
                report += f"- {log['timestamp'][:16]}: {log['content']}\n"
            report += "\n"
        
        if result_logs:
            report += "ğŸ“Š **ì‹¤í—˜ ê²°ê³¼:**\n"
            for log in result_logs[-5:]:
                report += f"- {log['timestamp'][:16]}: {log['content']}\n"
            report += "\n"
        
        if observation_logs:
            report += "ğŸ” **ê´€ì°° ì‚¬í•­:**\n"
            for log in observation_logs[-5:]:
                report += f"- {log['timestamp'][:16]}: {log['content']}\n"
            report += "\n"
        
        if issue_logs:
            report += "âš ï¸ **ì´ìŠˆ ë° ë¬¸ì œì :**\n"
            for log in issue_logs[-5:]:
                report += f"- {log['timestamp'][:16]}: {log['content']}\n"
            report += "\n"
        
        return report

# ì‹¤í—˜ ë¡œê±° ì¸ìŠ¤í„´ìŠ¤
experiment_logger = ExperimentLogger()

# LLM ê¸°ë°˜ ë©”ì‹œì§€ íƒ€ì… ë¶„ë¥˜ í•¨ìˆ˜
def llm_classify_message_type(message: str) -> str:
    """
    LLM(GPT-4o ë“±)ì„ ì‚¬ìš©í•´ ë©”ì‹œì§€ê°€ 'ì§ˆë¬¸'ì¸ì§€ 'ì‹¤í—˜ê¸°ë¡'ì¸ì§€ ë¶„ë¥˜í•œë‹¤.
    ë°˜ë“œì‹œ 'ì§ˆë¬¸' ë˜ëŠ” 'ì‹¤í—˜ê¸°ë¡' ë‘˜ ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µë³€í•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•œë‹¤.
    """
    llm = ChatOpenAI(model_name="gpt-4o-mini", openai_api_key=OPENAI_API_KEY, temperature=0)
    prompt = f"""
ì•„ë˜ ë©”ì‹œì§€ê°€ 'ì§ˆë¬¸'ì¸ì§€ 'ì‹¤í—˜ê¸°ë¡'ì¸ì§€ í•œ ë‹¨ì–´ë¡œ ë‹µí•´. 
ì§ˆë¬¸: ì‹¤í—˜ ë°©ë²•, ë§¤ë‰´ì–¼ ë“± ê¶ê¸ˆì¦. 
ì‹¤í—˜ê¸°ë¡: ì§„í–‰/ê´€ì°°/ê²°ê³¼/ì´ìŠˆ ë“±. 
ë°˜ë“œì‹œ 'ì§ˆë¬¸' ë˜ëŠ” 'ì‹¤í—˜ê¸°ë¡' ë‘˜ ì¤‘ í•˜ë‚˜ë¡œë§Œ ë‹µí•´. 
ë©”ì‹œì§€: {message}
"""
    result = llm.predict(prompt).strip().lower()
    # í˜¹ì‹œë¼ë„ LLMì´ ì—‰ëš±í•˜ê²Œ ë‹µí•  ê²½ìš° ë°©ì–´
    if "experiment" in result:
        return "experiment_log"
    return "question"

# ì‹¤í—˜ ë¡œê·¸ íƒ€ì… ë¶„ë¥˜
def classify_experiment_type(message: str) -> str:
    """ì‹¤í—˜ ë¡œê·¸ì˜ ì„¸ë¶€ íƒ€ì… ë¶„ë¥˜"""
    message_lower = message.lower()
    
    if any(keyword in message_lower for keyword in ["ê²°ê³¼", "ë°ì´í„°", "ì¸¡ì •ê°’", "ìˆ˜ì¹˜"]):
        return "result"
    elif any(keyword in message_lower for keyword in ["ê´€ì°°", "ë°œê²¬", "í™•ì¸", "ë³´ì˜€ì–´"]):
        return "observation"
    elif any(keyword in message_lower for keyword in ["ë¬¸ì œ", "ì´ìŠˆ", "ì‹¤íŒ¨", "ì˜¤ë¥˜", "ì•ˆë¨"]):
        return "issue"
    else:
        return "progress"

# manual_idë¡œ ë²¡í„°DBì—ì„œ ê²€ìƒ‰í•˜ëŠ” Tool ì •ì˜
def get_manual_search_tool(manual_id):
    def search_manual_func(input_text: str) -> str:
        print(f"[Tool] input_text: {input_text}")
        print(f"[Tool] manual_id: {manual_id}")
        start = time.time()
        embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
        vectorstore = Chroma(persist_directory=CHROMA_DIR, embedding_function=embeddings)
        docs = vectorstore.similarity_search(input_text, k=4, filter={"manual_id": manual_id})
        elapsed = time.time() - start
        print(f"[Tool] ê²€ìƒ‰ ì‹œê°„: {elapsed:.2f}ì´ˆ")
        print(f"[Tool] ê²€ìƒ‰ëœ ë¬¸ì„œ ê°œìˆ˜: {len(docs)}")
        if not docs:
            return "ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        return "\n".join([doc.page_content for doc in docs])
    return Tool(
        name=f"manual_search_{manual_id}",
        func=search_manual_func,
        description=f"{manual_id} ë§¤ë‰´ì–¼ì—ì„œ ê²€ìƒ‰í•©ë‹ˆë‹¤."
    )

def agent_chat_answer(manual_id: str, question: str, user_id: str = "default_user", history: List[Dict[str, str]] = None) -> Dict[str, str]:
    """
    ê°œì„ ëœ ì—ì´ì „íŠ¸ ë‹µë³€ í•¨ìˆ˜ (LLM ê¸°ë°˜ ë©”ì‹œì§€ ë¶„ë¥˜)
    Returns: {"response": str, "type": str, "logged": bool}
    """
    if history is None:
        history = []
    
    # === LLM ê¸°ë°˜ ë©”ì‹œì§€ íƒ€ì… ë¶„ë¥˜ ===
    message_type = llm_classify_message_type(question)
    
    if message_type == "experiment_log":
        # ì‹¤í—˜ ë¡œê·¸ë¡œ ì²˜ë¦¬
        exp_type = classify_experiment_type(question)
        log_entry = experiment_logger.add_experiment_log(user_id, question, exp_type)
        
        # ì‹¤í—˜ ë¡œê·¸ì— ëŒ€í•œ ì‘ë‹µ ìƒì„±
        responses = {
            "progress": [
                "ì‹¤í—˜ ì§„í–‰ ìƒí™©ì„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤! ê³„ì† ì§„í–‰í•˜ì‹œê³  ê²°ê³¼ê°€ ë‚˜ì˜¤ë©´ ì•Œë ¤ì£¼ì„¸ìš”.",
                "ë„¤, ì‹¤í—˜ ì§„í–‰ ìƒí™©ì„ ì˜ ê¸°ë¡í•´ë‘ì—ˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ë‹¨ê³„ë„ í™”ì´íŒ…í•˜ì„¸ìš”!",
                "ì‹¤í—˜ ì§„í–‰ ìƒí™©ì´ ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤. í˜¹ì‹œ ì§„í–‰ ì¤‘ ê¶ê¸ˆí•œ ì ì´ ìˆìœ¼ë©´ ì–¸ì œë“  ë¬¼ì–´ë³´ì„¸ìš”."
            ],
            "result": [
                "ì‹¤í—˜ ê²°ê³¼ë¥¼ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤! í¥ë¯¸ë¡œìš´ ê²°ê³¼ë„¤ìš”. ì¶”ê°€ ë¶„ì„ì´ í•„ìš”í•˜ì‹œë©´ ì•Œë ¤ì£¼ì„¸ìš”.",
                "ê²°ê³¼ ë°ì´í„°ê°€ ì˜ ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ìŒ ì‹¤í—˜ì„ ê³„íší•´ë³´ì‹œëŠ” ê±´ ì–´ë–¨ê¹Œìš”?",
                "ì‹¤í—˜ ê²°ê³¼ë¥¼ ì„±ê³µì ìœ¼ë¡œ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼ í•´ì„ì— ë„ì›€ì´ í•„ìš”í•˜ì‹œë©´ ë§ì”€í•´ì£¼ì„¸ìš”."
            ],
            "observation": [
                "ê´€ì°° ë‚´ìš©ì„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤. ì¢‹ì€ ê´€ì°°ì´ë„¤ìš”! ì´ëŸ° ì„¸ì‹¬í•œ ê´€ì°°ì´ ì‹¤í—˜ì˜ ì„±ê³µ ë¹„ê²°ì…ë‹ˆë‹¤.",
                "ê´€ì°° ì‚¬í•­ì´ ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŸ° ë³€í™”ë“¤ì„ ì˜ ì²´í¬í•˜ì‹œëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.",
                "ê´€ì°° ê²°ê³¼ë¥¼ ì˜ ê¸°ë¡í•´ë‘ì—ˆìŠµë‹ˆë‹¤. ì¶”ê°€ë¡œ ê´€ì°°ëœ ì‚¬í•­ì´ ìˆìœ¼ë©´ ì–¸ì œë“  ë§ì”€í•´ì£¼ì„¸ìš”."
            ],
            "issue": [
                "ë¬¸ì œ ìƒí™©ì„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤. í•´ê²° ë°©ë²•ì„ ë§¤ë‰´ì–¼ì—ì„œ ì°¾ì•„ë³¼ê¹Œìš”? êµ¬ì²´ì ì¸ ë¬¸ì œë¥¼ ì•Œë ¤ì£¼ì‹œë©´ ë„ì›€ì„ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                "ì´ìŠˆ ì‚¬í•­ì´ ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤. ë¹„ìŠ·í•œ ë¬¸ì œì— ëŒ€í•œ í•´ê²°ì±…ì„ ì°¾ì•„ë³´ì‹œê¸¸ ì›í•˜ì‹œë©´ ë§ì”€í•´ì£¼ì„¸ìš”.",
                "ë¬¸ì œ ìƒí™©ì„ ì˜ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤. ì´ëŸ° ì´ìŠˆë“¤ë„ ì†Œì¤‘í•œ ì‹¤í—˜ ë°ì´í„°ì…ë‹ˆë‹¤. í•´ê²° ë°©ì•ˆì„ í•¨ê»˜ ì°¾ì•„ë³´ì‹œê² ì–´ìš”?"
            ]
        }
        
        import random
        response = random.choice(responses.get(exp_type, responses["progress"]))
        
        return {
            "response": response,
            "type": "experiment_log",
            "logged": True
        }
    else:
        # ì§ˆë¬¸ìœ¼ë¡œ ì²˜ë¦¬ - ê¸°ì¡´ RAG ë°©ì‹
        history_text = ""
        for turn in history[-10:]:
            if turn["role"] == "user":
                history_text += f"ì‚¬ìš©ì: {turn['content']}\n"
            elif turn["role"] == "assistant":
                history_text += f"AI: {turn['content']}\n"
        
        # ìµœê·¼ ì‹¤í—˜ ë¡œê·¸ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€
        recent_logs = experiment_logger.get_user_experiments(user_id, limit=5)
        experiment_context = ""
        if recent_logs:
            experiment_context = "\nìµœê·¼ ì‹¤í—˜ ì§„í–‰ ìƒí™©:\n"
            for log in recent_logs:
                experiment_context += f"- {log['timestamp'][:16]}: {log['content']}\n"
        
        system_prompt = f"""
ë„ˆëŠ” ì‹¤í—˜ì‹¤ ë§¤ë‰´ì–¼ QA ë„ìš°ë¯¸ì•¼.
manual_id {manual_id}ì— í•´ë‹¹í•˜ëŠ” ë§¤ë‰´ì–¼ë§Œ ê²€ìƒ‰í•´ì•¼ í•œë‹¤.
ë§¤ë‰´ì–¼ ë‚´ìš©ì„ ë²—ì–´ë‚˜ì§€ ë§ê³ , ëª¨ë¥´ëŠ” ê±´ ëª¨ë¥¸ë‹¤ê³  ë‹µí•´.

ì´ì „ ëŒ€í™”:
{history_text}

{experiment_context}

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ë§¤ë‰´ì–¼ì„ ê²€ìƒ‰í•´ì„œ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•´ì¤˜.
"""
        
        from langchain.prompts import ChatPromptTemplate
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "{input}")
        ])
        
        llm = ChatOpenAI(model_name="gpt-4.1-mini", openai_api_key=OPENAI_API_KEY)
        tool = get_manual_search_tool(manual_id)
        agent = initialize_agent(
            [tool],
            llm,
            agent=AgentType.OPENAI_FUNCTIONS,
            verbose=True
        )
        
        answer = agent.run(question)
        
        return {
            "response": answer.strip(),
            "type": "question",
            "logged": False
        }

# ì‹¤í—˜ ë³´ê³ ì„œ ìƒì„± í•¨ìˆ˜
def generate_experiment_report(user_id: str = "default_user") -> str:
    """ì‚¬ìš©ìì˜ ì‹¤í—˜ ë¡œê·¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë³´ê³ ì„œ ìƒì„±"""
    return experiment_logger.generate_report(user_id)

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸
    manual_id = "lab_manual_001"
    user_id = "researcher_001"
    
    # ì‹¤í—˜ ì§„í–‰ ìƒí™© ë¡œê·¸
    result1 = agent_chat_answer(manual_id, "PCR ì‹¤í—˜ ì‹œì‘í–ˆì–´ìš”", user_id)
    print("Response 1:", result1)
    
    # ì§ˆë¬¸
    result2 = agent_chat_answer(manual_id, "PCR ì˜¨ë„ëŠ” ëª‡ë„ë¡œ ì„¤ì •í•´ì•¼ í•˜ë‚˜ìš”?", user_id)
    print("Response 2:", result2)
    
    # ì‹¤í—˜ ê²°ê³¼ ë¡œê·¸
    result3 = agent_chat_answer(manual_id, "PCR ê²°ê³¼ê°€ ë‚˜ì™”ëŠ”ë° ë°´ë“œê°€ íë¦¿í•˜ê²Œ ë‚˜ì™”ì–´ìš”", user_id)
    print("Response 3:", result3)
    
    # ë³´ê³ ì„œ ìƒì„±
    report = generate_experiment_report(user_id)
    print("Report:", report)
